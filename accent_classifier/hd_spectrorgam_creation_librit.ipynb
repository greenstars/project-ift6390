{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "9z2Gfyc-RgdE",
    "outputId": "8b4aecab-6016-444f-c804-2151f4d8211d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2bPwDh37Rhx1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M6pk23f2RlPf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "aJmlXwoGU45g",
    "outputId": "b1036798-7158-4707-9f8b-e7c80d873211"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fk7rCnFbUfGp"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "l9ihaBTURnQ3",
    "outputId": "799ee3af-3049-4d3c-98c1-90a349a6697d"
   },
   "outputs": [],
   "source": [
    "!ls /Users/vikuo/Documents/accent-classification-corpora\n",
    "librispeech_path = '/Users/vikuo/Documents/accent-classification-corpora/librispeech'\n",
    "librispeech_preprocessed_path = '/Users/vikuo/Documents/accent-classification-corpora/librispeech_preprocessed'\n",
    "librit_path = '/Users/vikuo/Documents/accent-classification-corpora/librit'\n",
    "librit_preprocessed_path = '/Users/vikuo/Documents/accent-classification-corpora/librit_preprocessed'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "53dTP_r0R_Ce"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import scipy.io.wavfile as wav\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "#matplotlib.use(\"TkAgg\")\n",
    "# Vi Comment: if i don't call this line, matplotlib crashes and gives me a call\n",
    "# stack trace that triggers *literally and figuratively* me and my computer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy.io.wavfile import read\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "6nBiXiZOUhvQ",
    "outputId": "d1d4fd81-647c-4265-9db6-b071498a1130"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SXoZQqVl3-Zu"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "MWZtKjqaRoxD",
    "outputId": "254efe3d-68fd-43a9-9857-a0f47c0827df"
   },
   "outputs": [],
   "source": [
    "\n",
    "from functools import reduce\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from pydub.utils import make_chunks\n",
    "\n",
    "import math\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "def remove_silence(audio):\n",
    "\t# Lightly adapted from:\n",
    "\t# https://stackoverflow.com/questions/23730796/using-pydub-to-chop-up-a-long-audio-file\n",
    "\t# We consider it silent if quieter than -16 dBFS for at least half a second.\n",
    "\t# (Might not use this if we want to match up with time stamps from transcriptions.)\n",
    "\t# Also it doesn't work right now anyway - debug later.\n",
    "\taudio_parts = split_on_silence(audio, min_silence_len=500, silence_thresh=-16)\n",
    "\taudio = reduce(lambda a, b: a + b, audio_parts)\t# Re-combine\n",
    "\treturn audio\n",
    "\n",
    "def stereo_to_mono(audio):\n",
    "\tmono_audio = audio.set_channels(1)\n",
    "\treturn mono_audio\n",
    "\n",
    "def get_speaker_dict(input_dir, corpus):\n",
    "\t# Group together the files belonging to each speaker\n",
    "\tfiles = [f for f in os.listdir(input_dir) if os.path.isfile(os.path.join(input_dir, f))]\n",
    "\tspeaker_files = {}\n",
    "\tfor file in files:\n",
    "\t\tif corpus == 'librispeech':\n",
    "\t\t\tspeaker_id = file.split('-')[0]\n",
    "\t\telif corpus == 'librit':\n",
    "\t\t\tspeaker_id = file.split('_')[0]\n",
    "\t\tif speaker_id in speaker_files:\n",
    "\t\t\tspeaker_files[speaker_id].append(file)\n",
    "\t\telse:\n",
    "\t\t\tspeaker_files[speaker_id] = [file]\n",
    "\treturn speaker_files\n",
    "\n",
    "def downsample(audio, sample_rate):\n",
    "\t# Downsamples an audio object to a given rate\n",
    "\taudio = audio.set_frame_rate(sample_rate)\n",
    "\treturn audio\n",
    "\n",
    "def consolidate_speakers(input_dir, speaker_id, speaker_dict):\n",
    "\t# Concatenates all files from specific speaker into one audio object.\n",
    "\tfile_list = speaker_dict[speaker_id]\n",
    "\tconsolidated = AudioSegment.empty()\n",
    "\tfor i, file in enumerate(file_list):\n",
    "\t\tprint('\tProcessing file {} of {}...'.format(i, len(file_list)))\n",
    "\t\tfile_audio = AudioSegment.from_wav(os.path.join(input_dir, file))\n",
    "\t\tconsolidated += file_audio\n",
    "\treturn consolidated\n",
    "\n",
    "def preprocess_violet(input_dir, output_dir, length_in_sec, corpus):\n",
    "\tspeaker_dict = get_speaker_dict(input_dir, corpus)\n",
    "\tspeaker_ids = list(speaker_dict.keys())\n",
    "\tfor j, speaker_id in enumerate(speaker_ids):\n",
    "\t\tprint('Making files for speaker {} ({} of {})...'.format(speaker_id, j, len(speaker_ids)))\n",
    "\t\t# Get the concatenated audio for each speaker\n",
    "\t\tspeaker_audio = consolidate_speakers(input_dir, speaker_id, speaker_dict)\n",
    "\t\t# Remove silence\n",
    "\t\t#speaker_audio = remove_silence(speaker_audio)\n",
    "\t\t# Downsample to mono\n",
    "\t\tspeaker_audio = stereo_to_mono(speaker_audio)\n",
    "\t\t# Librit needs to be downsampled to 16k\n",
    "\t\tif corpus == 'librit':\n",
    "\t\t\tspeaker_audio = downsample(speaker_audio, 16000)\n",
    "\t\t# Split into clips of length\n",
    "\t\tlength = length_in_sec * 1000\n",
    "\t\taudio_chunks = make_chunks(speaker_audio, length)\n",
    "\t\t# Export the clips\n",
    "\t\tfor i, chunk in enumerate(audio_chunks):\n",
    "\t\t\tchunk.export(os.path.join(output_dir, '{}_{}.wav'.format(speaker_id, i)), format='wav')\n",
    "      \n",
    "      \n",
    "def preprocess(input_dir, output_dir, length_in_sec, corpus):\n",
    "  speaker_dict = get_speaker_dict(input_dir, corpus)\n",
    "  speaker_ids = list(speaker_dict.keys())\n",
    "\n",
    "\n",
    "  file_count = 0\n",
    "  for speaker_id in tqdm(speaker_ids):\n",
    "    #print('Making files for speaker {} ({} of {})...'.format(speaker_id, j, len(speaker_ids)))\n",
    "    # Get the concatenated audio for each speaker\n",
    "    speaker_audio = consolidate_speakers(input_dir, speaker_id, speaker_dict)\n",
    "    # Remove silence\n",
    "    #speaker_audio = remove_silence(speaker_audio)\n",
    "    # Downsample to mono\n",
    "    speaker_audio = stereo_to_mono(speaker_audio)\n",
    "    # Librit needs to be downsampled to 16k\n",
    "    if corpus == 'librit':\n",
    "      speaker_audio = downsample(speaker_audio, 16000)\n",
    "    # Split into clips of length\n",
    "    length = length_in_sec * 1000\n",
    "    audio_chunks = make_chunks(speaker_audio, length)\n",
    "    # Export the clips\n",
    "    for i, chunk in enumerate(audio_chunks):\n",
    "      file_count += 1\n",
    "      chunk.export(os.path.join(output_dir, '{}_{}.wav'.format(speaker_id, i)), format='wav')\n",
    "  print('file_count = ',file_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "iqhOvQeiRtSS",
    "outputId": "64e5288a-6f5b-45bc-950d-231d689b83ac"
   },
   "outputs": [],
   "source": [
    "corpus = \"librit\"\n",
    "input_dir = \"/Users/vikuo/Documents/accent-classification-corpora/\"+corpus\n",
    "output_dir = \"/Users/vikuo/Documents/accent-classification-corpora/librit_preprocessed_1sec\"\n",
    "clip_length = 1\n",
    "\n",
    "preprocess(input_dir, output_dir, clip_length, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "t0AUQyvLed8X",
    "outputId": "9878aeb9-41d3-4fb1-f7d4-46d199756d18"
   },
   "outputs": [],
   "source": [
    "!ls /content/drive/My\\ Drive/librit_preprocessed_1sec/*.wav | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "icdwr5CjSYas"
   },
   "outputs": [],
   "source": [
    "def extract_spectrogram_violet(input_dir, output_dir):\n",
    "    cnt = 0\n",
    "    print_every = 100\n",
    "    \n",
    "    # to fix the os.listdir having varing change\n",
    "    os_list = os.listdir(input_dir)\n",
    "    print(\"fixed os list: \", len(os_list))\n",
    "    for i, filename in enumerate(os_list):\n",
    "        cnt += 1\n",
    "        if filename.endswith('.wav'):\n",
    "            if cnt % print_every == 1:\n",
    "                print('file number {}'.format(cnt))\n",
    "                print('Getting spectrogram for {} ({} of {})'.format(filename, i, len(os.listdir(input_dir))))\n",
    "            filename_prefix = filename.split('.')[0]\n",
    "            input_file_path = os.path.join(input_dir, filename)\n",
    "            x_value = 0\n",
    "            sr_value = 0\n",
    "            sr_value, x_value = read(input_file_path)\n",
    "            img = 0\n",
    "\n",
    "            # Get the spectrogram\n",
    "            #spectrum, specs, t, img = plt.specgram(x_value, NFFT=80, Fs=16000, noverlap=40)\n",
    "            spectrum, specs, t, img = plt.specgram(x_value, NFFT=400, Fs=16000, noverlap=160)\n",
    "            #plt.ylim([0,2000])\n",
    "            #plt.xlim([0,1])\n",
    "            #plt.show()\n",
    "            #print(spectrum.shape)\n",
    "            plt.clf() # Clear plot - this is necessary!\n",
    "            \n",
    "            \n",
    "            #stop\n",
    "\n",
    "            # Dump to a dictionary\n",
    "            spect_dict ={}\n",
    "            #spect_dict['img'] = img\n",
    "            spect_dict['t'] = t\n",
    "            spect_dict['specs'] = specs\n",
    "            spect_dict['spectrum'] = spectrum\n",
    "            with open(os.path.join(output_dir, '{}.1sec_hdpickle'.format(filename_prefix)), \"wb\" ) as jar:\n",
    "                pickle.dump(spect_dict ,jar,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQNgT-bJ3x92"
   },
   "outputs": [],
   "source": [
    "def extract_spectrogram(input_dir, output_dir):\n",
    "    for filename in tqdm(os.listdir(input_dir)):\n",
    "        if filename.endswith('.wav'):\n",
    "            #print('Getting spectrogram for {} ({} of {})'.format(filename, i, len(os.listdir(input_dir))))\n",
    "            filename_prefix = filename.split('.')[0]\n",
    "            input_file_path = os.path.join(input_dir, filename)\n",
    "            x_value = 0\n",
    "            sr_value = 0\n",
    "            sr_value, x_value = read(input_file_path)\n",
    "            img = 0\n",
    "\n",
    "            # Get the spectrogram\n",
    "            #spectrum, specs, t, img = plt.specgram(x_value, NFFT=80, Fs=16000, noverlap=40)\n",
    "            spectrum, specs, t, img = plt.specgram(x_value, NFFT=400, Fs=16000, noverlap=160)\n",
    "            #plt.ylim([0,2000])\n",
    "            #plt.xlim([0,1])\n",
    "            #plt.show()\n",
    "            #print(spectrum.shape)\n",
    "            plt.clf() # Clear plot - this is necessary!\n",
    "            \n",
    "            \n",
    "            #stop\n",
    "\n",
    "            # Dump to a dictionary\n",
    "            spect_dict ={}\n",
    "            #spect_dict['img'] = img\n",
    "            spect_dict['t'] = t\n",
    "            spect_dict['specs'] = specs\n",
    "            spect_dict['spectrum'] = spectrum\n",
    "            with open(os.path.join(output_dir, '{}.1sec_hdpickle'.format(filename_prefix)), \"wb\" ) as jar:\n",
    "                pickle.dump(spect_dict ,jar,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hQYRi6ONRxPD"
   },
   "outputs": [],
   "source": [
    "librit_preprocessed_path = \"/Users/vikuo/Documents/accent-classification-corpora/librit_preprocessed_1sec\"\n",
    "\n",
    "librit_spectrogram_path = \"/Users/vikuo/Documents/accent-classification-corpora/librit_1sec_hdpickle\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7769
    },
    "colab_type": "code",
    "id": "zOSpfPfES-lP",
    "outputId": "914cdeee-adde-4312-dbf4-23fbb33fcda8"
   },
   "outputs": [],
   "source": [
    "input_dir = librit_preprocessed_path # librispeech_preprocessed_path\n",
    "output_dir = librit_spectrogram_path # librispeech_spectrogram_path\n",
    "extract_spectrogram(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cXZ7_TLGUAXS",
    "outputId": "1a252713-cdc3-4d35-874f-82708080bba9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FH8hgZoyC9V1",
    "outputId": "22eee660-e823-42f5-bb42-bb29986146ca"
   },
   "outputs": [],
   "source": [
    "!ls /content/drive/My\\ Drive/librit_preprocessed_1sec/*.wav | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "du4137WqFQIR"
   },
   "outputs": [],
   "source": [
    "!ls /content/drive/My\\ Drive/librit_1sec_hdpickle | wc -l"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hd_spectrorgam_creation_librit.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
